\documentclass[usenames,dvipsnames,mathserif,notheorems]{beamer}

% silence annoying warnings
\usepackage{silence}
\usepackage{caption}
\WarningFilter{remreset}{The remreset package}
\usepackage{xcolor}

\input{macros/math}
\input{macros/plots}

\usepackage{simplebeam}
\usetheme{simplebeamer}

\usetikzlibrary{shapes, arrows}
\usetikzlibrary{decorations.pathreplacing, calligraphy}

% node styles
\tikzstyle{Input}=[minimum size=0.3cm, fill=black, line width = 0.5mm, draw=black, shape=circle, text=black]
\tikzstyle{Hidden}=[minimum size=0.3cm, fill=blue, line width = 0.5mm, draw=blue, shape=circle, text=black]
\tikzstyle{Splits}=[inner sep=0.03cm, minimum size=0.3cm, line width = 0.3mm, draw=blue, shape=circle, text=black]
\tikzstyle{Output}=[minimum size=0.3cm, fill=white, line width = 0.5mm, draw=black, shape=circle, text=black]

% Edge styles
\tikzstyle{arrow}=[line width = 0.5mm]

% bib resources

\addbibresource[]{refs.bib}

\title{Fast Convex Optimization for Two-Layer ReLU Networks:}
\subtitle{Equivalent Model Classes and Cone Decompositions}
\author{Aaron Mishkin \\ \texttt{amishkin@cs.stanford.edu}}
\collaborators{
		\includegraphics[width=0.2\linewidth]{assets/arda.jpg}
		\includegraphics[width=0.2\linewidth]{assets/mert.jpg}
    }

\titlegraphic{\includegraphics[width=0.4\textwidth]{assets/SUSig_2color_Stree_Left.eps}}

%\logo{\includegraphics[height=0.5cm]{assets/Block_S_2_color.eps}}

%\institute{Stanford University}
\date{}

\begin{document}

\maketitle
%% main content starts %%

\begin{frame}{Motivation: Neural Networks}

	Neural networks are building blocks for modern learning systems.
	\vspace{0.5em}
	\begin{itemize}
		\item \textbf{Vision}: object recognition, localization, and bounding.
		\item \textbf{Language}: audio transcription and machine translation.
		\item \textbf{Control}: robotics, autonomous cars, game-playing, \ldots
	\end{itemize}

	\vspace{0.5em}

	\begin{columns}
		\begin{column}{0.6\textwidth}
			\centering
			\textit{A bowl of soup that is a portal to another
				dimension as digital art.}
		\end{column}
		\begin{column}{0.45\textwidth}
			\begin{figure}[]
				\centering
				\includegraphics[width=0.8\linewidth]{assets/bowl_of_soup.jpg}
				\caption*{Generated by DALL$\cdot$E 2}%
			\end{figure}
		\end{column}
	\end{columns}

	\vspace{0.5em}

	\begin{center}
		\Large
		But optimizing neural networks is \textcolor{red}{hard}!
	\end{center}

	\source{https://openai.com/dall-e-2/}

\end{frame}

\begin{frame}{Motivation: Non-Convex Optimization}

	\begin{center}
		\Large
		DALL$\cdot$E 2 has 5.5 billion parameters and took billions of iterations to fit \citep{ramesh2022dalle}.
	\end{center}

	\vspace{1em}

	Neural network optimization is \textcolor{red}{non-convex}!
	\vspace{0.2em}
	\begin{itemize}
		\item \textbf{NP-Hard}: many sub-optimal local minima, saddles.
		\item \textbf{Speed/Stability}: tuning is critical for performance.
		\item \textbf{Model Churn}: hyper-parameters affect the final model~\citep{henderson2018deep}.
	\end{itemize}

	\begin{figure}[b]
		\centering
		\input{figures/non_convex}
	\end{figure}

\end{frame}

\begin{frame}{Convex Reformulations}

	\vspace{0.2em}
	{\large \textcolor{Red}{Non-Convex Problem}}
	\vspace{-1em}
	\begin{columns}
		\centering
		\begin{column}{0.2\linewidth}
			\small
			\[
				\begin{aligned}
					\min_{w, \alpha} & \norm{\sum_{j=1}^m (X w_j)_+ \alpha_j - y}_2^2             \\
					                 & \quad + \lambda \sum_{j=1}^m \norm{w_j}_2^2 + |\alpha_j|^2
				\end{aligned}
			\]
		\end{column}

		\begin{column}{0.7\linewidth}
			\begin{figure}[t]
				\raggedleft
				\input{figures/neural_net}
			\end{figure}
		\end{column}
	\end{columns}


	\pause

	{
		\vspace{-0.5em}
		\center \rule{\textwidth}{0.1em}
		\vspace{-0.2em}
	}

	{\large \textcolor{ForestGreen}{Convex Reformulation}}
	\vspace{-2em}
	\begin{columns}
		\begin{column}{0.2\linewidth}
			\vspace{1.5em}
			\small
			\[
				\begin{aligned}
					\min_{u} & \norm{\sum_{j=1}^p D_j X u_j - y}_2^2 +
					\lambda \sum_{j=1}^p \norm{u_j}_2,                 \\
					         & \hspace{0.2em} \text{where }
					D_j = \text{diag}[\mathbbm{1}(X g_i \geq 0)]
				\end{aligned}
			\]
		\end{column}
		\begin{column}{0.7\linewidth}
			\vspace{-1.5em}
			\begin{figure}[t]
				\raggedleft
				\input{figures/convex_reformulation}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Convex Reformulations: A Huge-Scale Linear Model}
	\vspace{-2em}
	\[
		\begin{aligned}
			\textbf{Convex Form}: \quad
			\min_{u} \; & \norm{\sum_{j=1}^p D_j X u_j - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{u_j}_2,                    \\
			            & \hspace{0.2em} \text{where }
			D_j = \text{diag}[\mathbbm{1}(X g_i \geq 0)]
		\end{aligned}
	\]
	\begin{itemize}
		\item \textcolor{red}{Exponential in general}: \( p \in O(r \cdot (\frac{n}{r})^r) \),
		      where \( r = \text{rank}(X) \).
		      \vspace{0.2em}

		\item But, \( D_j X \) is \textcolor{ForestGreen}{row-sparse}, \( u \) is \textcolor{ForestGreen}{sparse} when
		      \( \lambda \gg 0 \), and the objective is \textcolor{ForestGreen}{quadratic} + simple non-smooth.
	\end{itemize}

	\pause

	{
		\vspace{-0.5em}
		\center \rule{\textwidth}{0.1em}
		\vspace{-0.2em}
	}
	Solve with (accelerated) \textbf{proximal-gradient} methods:
	\begin{align*}
		u_i^+
		 & = u_i^k - \etak X^\top D_i^\top \rbr{\sum_{j=1}^p D_j X u_j - y} \\
		u_i^{k+1}
		 & = u_i^+ \rbr{1 - \frac{\etak \cdot \lambda}{\norm{u_i^+}_2}}_+
	\end{align*}

\end{frame}

\begin{frame}{Convex Reformulations: Performance}

	Fast solvers use \textbf{numerical tricks} and \textbf{hardware acceleration}:
	\vspace{1em}
	\begin{itemize}
		\item \textcolor{ForestGreen}{Classic Tricks}: faster convergence via line-search, restarts, \ldots
		\item \textcolor{ForestGreen}{CUDA GPUs}: \( 70\times \) faster Mat-Vec operations using \texttt{float32}.
		\item \textcolor{ForestGreen}{Code Optimization}: tensor operations remove intermediate computations,
		      data normalization improves conditioning, \ldots
	\end{itemize}
	\vspace{1em}

	\textcolor{red}{Scaling} is a still a problem!
	\begin{itemize}
		\item Dense operations on GPUs are faster than sparse computations with OpenMP --- does this reverse at scale?
		\item GPU memory is typically 32GB, but ImageNet is 150GB --- can we use multi-GPU programming models?
		\item How can we leverage the (dynamic) sparsity pattern of \( u^k \)?
	\end{itemize}

\end{frame}

%% main content ends %%

%% end slide
\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge Thanks for Listening!
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Bonus: Numerical Results (1)}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.75\linewidth]{figures/pp_acceleration.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Bonus: Numerical Results (2)}
	\begin{figure}[t]
		\centering
		\includegraphics[width=1\linewidth]{figures/pp_main.pdf}
	\end{figure}
\end{frame}

%% bibliography
\begin{frame}[allowframebreaks]{References}
	\printbibliography[]
\end{frame}


\end{document}
