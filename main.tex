\documentclass[usenames,dvipsnames,mathserif,notheorems]{beamer}

% silence annoying warnings
\usepackage{silence}
\usepackage{caption}
\WarningFilter{remreset}{The remreset package}
\usepackage{xcolor}

\input{macros/math}
\input{macros/plots}

\usepackage{simplebeam}
\usetheme{simplebeamer}

\usetikzlibrary{shapes, arrows}
\usetikzlibrary{decorations.pathreplacing, calligraphy}

% node styles
\tikzstyle{Input}=[minimum size=0.3cm, fill=black, line width = 0.5mm, draw=black, shape=circle, text=black]
\tikzstyle{Hidden}=[minimum size=0.3cm, fill=blue, line width = 0.5mm, draw=blue, shape=circle, text=black]
\tikzstyle{Splits}=[inner sep=0.03cm, minimum size=0.3cm, line width = 0.3mm, draw=blue, shape=circle, text=black]
\tikzstyle{Output}=[minimum size=0.3cm, fill=white, line width = 0.5mm, draw=black, shape=circle, text=black]

% Edge styles
\tikzstyle{arrow}=[line width = 0.5mm]

% bib resources

\addbibresource[]{refs.bib}

\title{Fast Convex Optimization for Two-Layer ReLU Networks:}
\subtitle{Equivalent Model Classes and Cone Decompositions}
\author{Aaron Mishkin \and Arda Sahiner \and Mert Pilanci}
%\institute{Stanford University}
\collaborators{
		\includegraphics[width=0.2\linewidth]{assets/aaron.png}
		\includegraphics[width=0.2\linewidth]{assets/arda.jpg}
		\includegraphics[width=0.2\linewidth]{assets/mert.jpg}
    }

\titlegraphic{\includegraphics[width=0.4\textwidth]{assets/SUSig_2color_Stree_Left.eps}}

\newcommand{\centerrule}{
	{
	\vspace{-0.5em}
	\center \rule{\textwidth}{0.1em}
	\vspace{-0.2em}
	}
}

%\logo{\includegraphics[height=0.5cm]{assets/Block_S_2_color.eps}}

%\institute{Stanford University}
\date{}

\begin{document}

\maketitle
%% main content starts %%

\begin{frame}{Overview}

	{
		\large \textcolor{red}{Problem}: Training shallow neural networks is challenging.
	}
	\vspace{0.5em}

	\pause
	\begin{itemize}
		\item \textbf{Tuning}: step-size and other hyper-parameters must be tuned.
		      \pause
		\item \textbf{Model Churn}: models trained with different random seeds
		      have different performance \citep{henderson2018deep}.
		      \pause
		\item \textbf{Certificates}: final models have few guarantees.
	\end{itemize}

	\pause

	\centerrule
	{
		\large \textcolor{ForestGreen}{Our Contribution}: robust training by convex reformulations.
	}
	\vspace{0.5em}
	\pause
	\begin{itemize}
		\item We develop new convex reformulations of two-layer neural networks
		      with \textbf{gated ReLU} activations.
		      \pause
		\item We show how to approximate the ReLU training problem by \textbf{unconstrained}
		      convex optimization of a Gated ReLU network.
		      \pause
		\item We propose and \textbf{exhaustively evaluate} algorithms for solving
		      our convex reformulations.
	\end{itemize}

\end{frame}

\begin{frame}{Background on Convex Reformulations}

	\vspace{0.2em}
	{\large \textcolor{Red}{Non-Convex Problem}}
	\vspace{-1em}
	\begin{columns}
		\centering
		\begin{column}{0.2\linewidth}
			\small
			\[
				\begin{aligned}
					\min_{W} & \norm{\sum_{j=1}^m (X W_{1j})_+ w_{2j} - y}_2^2                  \\
					         & \quad + \lambda \sum_{j=1}^m \norm{W_{1j}}_2^2 + \norm{w_{2j}}^2
				\end{aligned}
			\]
		\end{column}

		\begin{column}{0.7\linewidth}
			\begin{figure}[t]
				\raggedleft
				\input{assets/neural_net}
			\end{figure}
		\end{column}
	\end{columns}


	\pause
	\centerrule


	{\large \textcolor{ForestGreen}{Convex Reformulation}} \citep{pilanci2020convex}
	\vspace{-2em}
	\begin{columns}
		\begin{column}{0.2\linewidth}
			\vspace{1.5em}
			\small
			\[
				\begin{aligned}
					\min_{v, w} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2                       \\
					            & \hspace{0.1em } + \lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2, \\
					            & \hspace{0.1em} \text{s.t. }
					v_j, w_j \in \calK_j \text{ for } j = 1,\ldots,p.
				\end{aligned}
			\]
		\end{column}
		\begin{column}{0.7\linewidth}
			\vspace{-1.5em}
			\begin{figure}[t]
				\raggedleft
				\input{assets/convex_reformulation}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Background on Convex Reformulations}

	\vspace{0.2em}
	{\large Non-Convex Problem}
	\vspace{-1em}
	\begin{columns}
		\centering
		\begin{column}{0.2\linewidth}
			\small
			\[
				\begin{aligned}
					\min_{W} & \norm{\sum_{j=1}^m (X W_{1j})_+ w_{2j} - y}_2^2                  \\
					         & \quad + \lambda \sum_{j=1}^m \norm{W_{1j}}_2^2 + \norm{w_{2j}}^2
				\end{aligned}
			\]
		\end{column}

		\begin{column}{0.7\linewidth}
			\begin{figure}[t]
				\raggedleft
				\input{assets/neural_net}
			\end{figure}
		\end{column}
	\end{columns}

	\centerrule

	{\large Convex Reformulation} \citep{pilanci2020convex}
	\vspace{-2em}
	\begin{columns}
		\begin{column}{0.2\linewidth}
			\vspace{1.5em}
			\small
			\[
				\begin{aligned}
					\min_{v, w} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2                       \\
					            & \hspace{0.1em } + \lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2, \\
					            & \hspace{0.1em}
					\textcolor{red}{\text{s.t. }
						v_j, w_j \in \calK_j \text{ for } j = 1,\ldots,p
					}.
				\end{aligned}
			\]
		\end{column}
		\begin{column}{0.7\linewidth}
			\vspace{-1.5em}
			\begin{figure}[t]
				\raggedleft
				\input{assets/convex_reformulation}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Gated ReLU Networks}

	\[
		\begin{aligned}
			\textcolor{Red}{\text{C-ReLU}}: \min_{v, w} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2
			+ \lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2,                                         \\
			                                            & \hspace{0.1em}
			\textcolor{red}{\text{s.t. }
				v_j, w_j \in \calK_j \text{ for } j = 1,\ldots,p
			}.
		\end{aligned}
	\]
	\pause
	\centerrule
	\[
		\begin{aligned}
			\textcolor{ForestGreen}{\text{C-GReLU}}: \min_{u} & \norm{\sum_{j=1}^p D_j X u_j - y}_2^2
			+ \lambda \sum_{j=1}^p \norm{u_j}_2,                                                      \\
		\end{aligned}
	\]
	\pause
    \vspace{1em}

	\textbf{Prop.} (informal): C-GReLU is equivalent to a ``gated ReLU'' network \citep{fiat2019decoupling}
	with activation function
	\[ \phi_{g}(X, u) = \text{diag}(\mathbbm{1}(Xg \geq 0)) X u. \]

\end{frame}

\begin{frame}{Gated ReLU: Cone Decompositions}
	\begin{itemize}
		\item We reparameterized as \( \textcolor{ForestGreen}{u_j} = \textcolor{Red}{v_j - w_j} \). \pause
		\item Given, \( \textcolor{ForestGreen}{u_j} \), can we go back to \( \textcolor{Red}{v_j - w_j} \)? \pause
		\item That is, when does \( \textcolor{Red}{\calK_j - \calK_j} \) span \( \R^d \)? \pause
	\end{itemize}

	\begin{figure}[]
		\centering
		\input{assets/cone_decomp}
	\end{figure}
	\pause

	\begin{center}
		\textbf{Informal Result}: \( \textcolor{Red}{\calK_j - \calK_j} = \R^d \) or \( \calK_j \) is ``unimportant''.
	\end{center}
\end{frame}

\begin{frame}{Main Approximation Result}
	\begin{center}
		We can decompose a Gated ReLU neuron into two ReLU neurons.
	\end{center}
	\pause
	\begin{theorem}[Approximation by Cone Decomposition]
		Let \( \lambda \geq 0 \) and let \( p^* \) be the optimal value of the ReLU problem.
		There exists a C-GReLU problem with minimizer \( u^* \) and optimal value \( d^* \) satisfying,
		\[
			d^* \leq p^* \leq d^* + \textcolor{Red}{2 \lambda \kappa(\tilde X_{\calJ}) \sum_{D_i \in \tilde \calD} \norm{u_i^*}_2}.
		\]
	\end{theorem}
	\pause

	{\large Additional Consequences}
	\begin{itemize}
		\item The approximation is exact for \textbf{unregularized} models!
		\item The Gated ReLU and ReLU models are formally \textbf{equivalent}!
	\end{itemize}
\end{frame}

\begin{frame}{Solving the Convex Programs}
	\begin{figure}[]
		\centering
		\input{assets/relations}
	\end{figure}
	We develop two algorithms for solving the convex reformulations:
	\begin{itemize}
		\item \textbf{R-FISTA}: a restarted FISTA variant for Gated ReLU.
		\item \textbf{AL}: an augmented Lagrangian method for the (constrained) ReLU Problem.
	\end{itemize}
	Our work exhaustively evaluates the performance of R-FISTA and AL.
\end{frame}

\begin{frame}{Numerical Results}
	\begin{figure}[t]
		\centering
		\includegraphics[width=1\linewidth]{assets/pp_main.pdf}
	\end{figure}
	\begin{itemize}
		\item Generated by 438 training problems taken from UCI repo.
		\item R-FISTA/AL solve more, faster, than SGD and Adam.
	\end{itemize}
\end{frame}


%% main content ends %%

%% end slide
\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge Thanks for Listening!
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

%% bibliography
\begin{frame}[allowframebreaks]{References}
	\printbibliography[]
\end{frame}


\end{document}
