\documentclass[usenames,dvipsnames,mathserif,notheorems]{beamer}

% silence annoying warnings
\usepackage{silence}
\usepackage{caption}
\WarningFilter{remreset}{The remreset package}
\usepackage{xcolor}

\input{macros/math}
\input{macros/plots}

\usepackage{simplebeam}
\usetheme{simplebeamer}

\usetikzlibrary{shapes, arrows}
\usetikzlibrary{decorations.pathreplacing, calligraphy}

% node styles
\tikzstyle{Input}=[minimum size=0.3cm, fill=black, line width = 0.5mm, draw=black, shape=circle, text=black]
\tikzstyle{Hidden}=[minimum size=0.3cm, fill=blue, line width = 0.5mm, draw=blue, shape=circle, text=black]
\tikzstyle{Splits}=[inner sep=0.03cm, minimum size=0.3cm, line width = 0.3mm, draw=blue, shape=circle, text=black]
\tikzstyle{Output}=[minimum size=0.3cm, fill=white, line width = 0.5mm, draw=black, shape=circle, text=black]

% Edge styles
\tikzstyle{arrow}=[line width = 0.5mm]

% bib resources

\addbibresource[]{refs.bib}

\title{Fast Convex Optimization for Two-Layer ReLU Networks:}
\subtitle{Equivalent Model Classes and Cone Decompositions}
\author{Aaron Mishkin \\ \texttt{amishkin@cs.stanford.edu}}
\collaborators{
		\includegraphics[width=0.2\linewidth]{assets/arda.jpg}
		\includegraphics[width=0.2\linewidth]{assets/mert.jpg}
    }

\titlegraphic{\includegraphics[width=0.4\textwidth]{assets/SUSig_2color_Stree_Left.eps}}

%\logo{\includegraphics[height=0.5cm]{assets/Block_S_2_color.eps}}

%\institute{Stanford University}
\date{}

\begin{document}

\maketitle
%% main content starts %%

\begin{frame}{Motivation: Neural Networks}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.65\linewidth]{assets/bowl_of_soup.jpg}
		\caption*{Generated by DALL$\cdot$E 2}%
	\end{figure}

	\begin{center}
		\textit{\Large A bowl of soup that is a portal to another
			dimension as digital art.}
	\end{center}

	\source{https://openai.com/dall-e-2/}

\end{frame}

\begin{frame}{Motivation: Non-Convex Optimization}

	\begin{center}
		\Large
		DALL$\cdot$E 2 has 5.5 billion parameters and took billions of iterations to fit \citep{ramesh2022dalle}.

	\end{center}

	\vspace{1em}
	\pause

	{
		\vspace{-0.5em}
		\center \rule{\textwidth}{0.1em}
		\vspace{-0.2em}
	}

	\begin{center}
		\Large

		\textbf{Main Challenge}: neural networks are \textcolor{red}{non-convex}!
	\end{center}


	\vspace{0.2em}
	\begin{figure}[b]
		\centering
		\input{figures/non_convex}
	\end{figure}

\end{frame}

\begin{frame}{Motivation: Training Pathologies}
	\begin{center}
		\Large
		Optimizing neural networks with stochastic gradient descent is \textcolor{red}{hard}!
	\end{center}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.9\linewidth]{figures/synthetic_classification.pdf}
	\end{figure}

\end{frame}

\begin{frame}{Convex Reformulations: Non-Convex Problem}

	{\large \textcolor{Red}{Non-Convex Problem}}
	\[
		\min_{w, \alpha} \norm{\sum_{j=1}^m (X w_j)_+ \alpha_j - y}_2^2
		+ \lambda \sum_{j=1}^m \norm{w_j}_2^2 + |\alpha_j|^2
	\]

	\begin{figure}[]
		\centering
		\input{figures/neural_net}
	\end{figure}

\end{frame}
\begin{frame}{Convex Reformulations: Convex Problem}

	{\large \textcolor{ForestGreen}{Convex Reformulation}}
	\[
		\begin{aligned}
			\min_{u} & \norm{\sum_{j=1}^p D_j X u_j - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{u_j}_2,                 \\
			         & \hspace{0.2em} \text{where }
			D_j = \text{diag}[\mathbbm{1}(X g_i \geq 0)]
		\end{aligned}
	\]
	\begin{figure}[]
		\centering
		\input{figures/convex_reformulation}
	\end{figure}
\end{frame}

\begin{frame}{Convex Reformulations: A Huge-Scale Linear Model}
	\vspace{-2em}
	\[
		\begin{aligned}
			\textbf{Convex Form}: \quad
			\min_{u} \; & \norm{\sum_{j=1}^p D_j X u_j - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{u_j}_2,                    \\
			            & \hspace{0.2em} \text{where }
			D_j = \text{diag}[\mathbbm{1}(X g_i \geq 0)]
		\end{aligned}
	\]
	{
	\vspace{-0.5em}
	\center \rule{\textwidth}{0.1em}
	\vspace{-0.2em}
	}

	\begin{itemize}
		\Large
		\item \textcolor{red}{Exponential in general}: \( p \in O(r \cdot (\frac{n}{r})^r) \),
		      where \( r = \text{rank}(X) \).
		      \vspace{1em}

		\item But the problem is highly \textcolor{ForestGreen}{structured}!
	\end{itemize}

\end{frame}

\begin{frame}{Convex Reformulations: Performance}

	{\large Fast solvers use \textbf{numerical tricks} and \textbf{hardware acceleration}:}
	\vspace{1em}
	\begin{itemize}
		\item \textcolor{ForestGreen}{Classic Tricks}: faster convergence via line-search, restarts, \ldots
		\item \textcolor{ForestGreen}{CUDA GPUs}: \( 70\times \) faster Mat-Vec operations using \texttt{float32}.
	\end{itemize}
	\vspace{1em}

	\textcolor{red}{Scaling} is a still a problem!
	\begin{itemize}
		\item Dense operations on GPUs are faster than sparse computations.
		\item GPU memory is limited --- can we use multi-GPU programming models?
	\end{itemize}

\end{frame}
\begin{frame}{Results: Convex vs Non-Convex}

	\begin{center}
		{\Large The convex solver is \textcolor{ForestGreen}{stable!}}
	\end{center}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.9\linewidth]{figures/synthetic_classification.pdf}
	\end{figure}
\end{frame}

%% main content ends %%

%% end slide
\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge Thanks for Listening!
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Bonus: Numerical Results (1)}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.75\linewidth]{figures/pp_acceleration.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Bonus: Numerical Results (2)}
	\begin{figure}[t]
		\centering
		\includegraphics[width=1\linewidth]{figures/pp_main.pdf}
	\end{figure}
\end{frame}

%% bibliography
\begin{frame}[allowframebreaks]{References}
	\printbibliography[]
\end{frame}


\end{document}
